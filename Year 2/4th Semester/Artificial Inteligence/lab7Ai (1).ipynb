{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "J7tl9AwIZMyT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "INPUT_FOLDER = \"images/original\"\n",
        "SEPIA_FOLDER = \"images/sepia\"\n",
        "NORMAL_FOLDER = \"images/normal\"\n",
        "\n",
        "os.makedirs(SEPIA_FOLDER, exist_ok=True)\n",
        "os.makedirs(NORMAL_FOLDER, exist_ok=True)\n",
        "\n",
        "def apply_sepia(image):\n",
        "    sepia_filter = np.array([[0.393, 0.769, 0.189],\n",
        "                             [0.349, 0.686, 0.168],\n",
        "                             [0.272, 0.534, 0.131]])\n",
        "    img = np.array(image)\n",
        "    sepia_img = img @ sepia_filter.T\n",
        "    sepia_img = np.clip(sepia_img, 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(sepia_img)\n",
        "\n",
        "#itereaza peste imagini\n",
        "for filename in os.listdir(INPUT_FOLDER):\n",
        "    image_path = os.path.join(INPUT_FOLDER, filename)\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize((64, 64))  #uniformzare\n",
        "    #originalul\n",
        "    image.save(os.path.join(NORMAL_FOLDER, filename))\n",
        "    #varianta cu sepia\n",
        "    sepia_image = apply_sepia(image)\n",
        "    sepia_image.save(os.path.join(SEPIA_FOLDER, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(normal_path=\"images/normal\", sepia_path=\"images/sepia\"):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    #imagini normale (eticheta 0)\n",
        "    for file in os.listdir(normal_path):\n",
        "        img = Image.open(os.path.join(normal_path, file)).convert(\"RGB\")\n",
        "        X.append(np.array(img).flatten() / 255.0)  # scalare 0-1\n",
        "        y.append(0)\n",
        "\n",
        "    #imagini sepia (eticheta 1)\n",
        "    for file in os.listdir(sepia_path):\n",
        "        img = Image.open(os.path.join(sepia_path, file)).convert(\"RGB\")\n",
        "        X.append(np.array(img).flatten() / 255.0)\n",
        "        y.append(1)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "d5h6ONBmaF8w"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN TOOL"
      ],
      "metadata": {
        "id": "mI8Jqsk9gKB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Parametri\n",
        "IMG_SIZE = (64, 64)\n",
        "normal_path = \"images/normal\"\n",
        "sepia_path = \"images/sepia\"\n",
        "\n",
        "def load_images_from_folder(folder, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            img_path = os.path.join(folder, filename)\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img = img.resize(IMG_SIZE)\n",
        "            img_array = np.array(img) / 255.0  # normalizare\n",
        "            data.append((img_array, label))\n",
        "    return data\n",
        "\n",
        "normal_data = load_images_from_folder(normal_path, 0)\n",
        "sepia_data = load_images_from_folder(sepia_path, 1)\n",
        "\n",
        "all_data = normal_data + sepia_data\n",
        "np.random.shuffle(all_data)\n",
        "\n",
        "#imagini si etichete\n",
        "X = np.array([item[0] for item in all_data])\n",
        "y = np.array([item[1] for item in all_data])\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y unique values:\", np.unique(y))\n",
        "\n",
        "#impartire date\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train label distribution:\", np.unique(y_train, return_counts=True))\n",
        "print(\"Test label distribution:\", np.unique(y_test, return_counts=True))\n",
        "\n",
        "# Aplatizare imagini\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Definire si antrenare model\n",
        "model = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model.fit(X_train_flat, y_train)\n",
        "\n",
        "# Predictii si evaluare\n",
        "y_pred = model.predict(X_test_flat)\n",
        "print(\"Distribu»õie predictii:\", np.unique(y_pred, return_counts=True))\n",
        "\n",
        "print(\"\\nClasificare pe setul de test:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"Acuratete: {accuracy_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "dq8VtCNyapUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999624c1-3d93-43c1-b27f-1b233427bdd6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (40, 64, 64, 3)\n",
            "y unique values: [0 1]\n",
            "Train label distribution: (array([0, 1]), array([16, 16]))\n",
            "Test label distribution: (array([0, 1]), array([4, 4]))\n",
            "Iteration 1, loss = 0.71302696\n",
            "Iteration 2, loss = 3.36285012\n",
            "Iteration 3, loss = 4.66495313\n",
            "Iteration 4, loss = 2.84783949\n",
            "Iteration 5, loss = 0.79543790\n",
            "Iteration 6, loss = 1.83837648\n",
            "Iteration 7, loss = 1.11391227\n",
            "Iteration 8, loss = 1.04124553\n",
            "Iteration 9, loss = 1.32096462\n",
            "Iteration 10, loss = 0.67506927\n",
            "Iteration 11, loss = 0.93623411\n",
            "Iteration 12, loss = 1.00352733\n",
            "Iteration 13, loss = 0.64122508\n",
            "Iteration 14, loss = 0.78524071\n",
            "Iteration 15, loss = 0.90624961\n",
            "Iteration 16, loss = 0.64522137\n",
            "Iteration 17, loss = 0.66261643\n",
            "Iteration 18, loss = 0.80049392\n",
            "Iteration 19, loss = 0.64598132\n",
            "Iteration 20, loss = 0.59104821\n",
            "Iteration 21, loss = 0.72126857\n",
            "Iteration 22, loss = 0.61846431\n",
            "Iteration 23, loss = 0.56205451\n",
            "Iteration 24, loss = 0.64950168\n",
            "Iteration 25, loss = 0.57638154\n",
            "Iteration 26, loss = 0.54092410\n",
            "Iteration 27, loss = 0.60103451\n",
            "Iteration 28, loss = 0.53234013\n",
            "Iteration 29, loss = 0.52512523\n",
            "Iteration 30, loss = 0.55925099\n",
            "Iteration 31, loss = 0.51275442\n",
            "Iteration 32, loss = 0.50678915\n",
            "Iteration 33, loss = 0.52913971\n",
            "Iteration 34, loss = 0.49146074\n",
            "Iteration 35, loss = 0.48488380\n",
            "Iteration 36, loss = 0.49735885\n",
            "Iteration 37, loss = 0.46465808\n",
            "Iteration 38, loss = 0.46455301\n",
            "Iteration 39, loss = 0.47086126\n",
            "Iteration 40, loss = 0.44408706\n",
            "Iteration 41, loss = 0.44911026\n",
            "Iteration 42, loss = 0.44512413\n",
            "Iteration 43, loss = 0.42463742\n",
            "Iteration 44, loss = 0.43065699\n",
            "Iteration 45, loss = 0.42016276\n",
            "Iteration 46, loss = 0.40918715\n",
            "Iteration 47, loss = 0.41261001\n",
            "Iteration 48, loss = 0.39900063\n",
            "Iteration 49, loss = 0.39223184\n",
            "Iteration 50, loss = 0.39114139\n",
            "Iteration 51, loss = 0.37946102\n",
            "Iteration 52, loss = 0.38147224\n",
            "Iteration 53, loss = 0.41453699\n",
            "Iteration 54, loss = 0.36634274\n",
            "Iteration 55, loss = 0.38364248\n",
            "Iteration 56, loss = 0.36732382\n",
            "Iteration 57, loss = 0.35244216\n",
            "Iteration 58, loss = 0.36397500\n",
            "Iteration 59, loss = 0.33368598\n",
            "Iteration 60, loss = 0.34758630\n",
            "Iteration 61, loss = 0.32949070\n",
            "Iteration 62, loss = 0.31849883\n",
            "Iteration 63, loss = 0.31654278\n",
            "Iteration 64, loss = 0.30831813\n",
            "Iteration 65, loss = 0.31417382\n",
            "Iteration 66, loss = 0.34106486\n",
            "Iteration 67, loss = 0.32980583\n",
            "Iteration 68, loss = 0.44141713\n",
            "Iteration 69, loss = 0.31719187\n",
            "Iteration 70, loss = 0.35077209\n",
            "Iteration 71, loss = 0.36085621\n",
            "Iteration 72, loss = 0.27396706\n",
            "Iteration 73, loss = 0.34900204\n",
            "Iteration 74, loss = 0.26530411\n",
            "Iteration 75, loss = 0.29792820\n",
            "Iteration 76, loss = 0.26521635\n",
            "Iteration 77, loss = 0.27369561\n",
            "Iteration 78, loss = 0.25395723\n",
            "Iteration 79, loss = 0.25286223\n",
            "Iteration 80, loss = 0.23807105\n",
            "Iteration 81, loss = 0.23921124\n",
            "Iteration 82, loss = 0.22180931\n",
            "Iteration 83, loss = 0.22815090\n",
            "Iteration 84, loss = 0.21216888\n",
            "Iteration 85, loss = 0.21810445\n",
            "Iteration 86, loss = 0.20307772\n",
            "Iteration 87, loss = 0.20765925\n",
            "Iteration 88, loss = 0.19512416\n",
            "Iteration 89, loss = 0.19851096\n",
            "Iteration 90, loss = 0.18723890\n",
            "Iteration 91, loss = 0.18905027\n",
            "Iteration 92, loss = 0.17995402\n",
            "Iteration 93, loss = 0.18064431\n",
            "Iteration 94, loss = 0.17205102\n",
            "Iteration 95, loss = 0.17182628\n",
            "Iteration 96, loss = 0.16457903\n",
            "Iteration 97, loss = 0.16364338\n",
            "Iteration 98, loss = 0.15715208\n",
            "Iteration 99, loss = 0.15565145\n",
            "Iteration 100, loss = 0.15016111\n",
            "Iteration 101, loss = 0.14901873\n",
            "Iteration 102, loss = 0.14386187\n",
            "Iteration 103, loss = 0.14190199\n",
            "Iteration 104, loss = 0.13756327\n",
            "Iteration 105, loss = 0.13535323\n",
            "Iteration 106, loss = 0.13192542\n",
            "Iteration 107, loss = 0.12963698\n",
            "Iteration 108, loss = 0.12634999\n",
            "Iteration 109, loss = 0.12360536\n",
            "Iteration 110, loss = 0.12067146\n",
            "Iteration 111, loss = 0.11780185\n",
            "Iteration 112, loss = 0.11504228\n",
            "Iteration 113, loss = 0.11197218\n",
            "Iteration 114, loss = 0.10961622\n",
            "Iteration 115, loss = 0.10675032\n",
            "Iteration 116, loss = 0.10464310\n",
            "Iteration 117, loss = 0.10180704\n",
            "Iteration 118, loss = 0.09973121\n",
            "Iteration 119, loss = 0.09701523\n",
            "Iteration 120, loss = 0.09502115\n",
            "Iteration 121, loss = 0.09242468\n",
            "Iteration 122, loss = 0.09032882\n",
            "Iteration 123, loss = 0.08797679\n",
            "Iteration 124, loss = 0.08585634\n",
            "Iteration 125, loss = 0.08368282\n",
            "Iteration 126, loss = 0.08145337\n",
            "Iteration 127, loss = 0.07889317\n",
            "Iteration 128, loss = 0.07672631\n",
            "Iteration 129, loss = 0.07482857\n",
            "Iteration 130, loss = 0.07263486\n",
            "Iteration 131, loss = 0.07034521\n",
            "Iteration 132, loss = 0.06796899\n",
            "Iteration 133, loss = 0.06556514\n",
            "Iteration 134, loss = 0.06313681\n",
            "Iteration 135, loss = 0.06083169\n",
            "Iteration 136, loss = 0.05889294\n",
            "Iteration 137, loss = 0.05728459\n",
            "Iteration 138, loss = 0.05570671\n",
            "Iteration 139, loss = 0.05403414\n",
            "Iteration 140, loss = 0.05235225\n",
            "Iteration 141, loss = 0.05067044\n",
            "Iteration 142, loss = 0.04915488\n",
            "Iteration 143, loss = 0.04773129\n",
            "Iteration 144, loss = 0.04633650\n",
            "Iteration 145, loss = 0.04495830\n",
            "Iteration 146, loss = 0.04353820\n",
            "Iteration 147, loss = 0.04218953\n",
            "Iteration 148, loss = 0.04087019\n",
            "Iteration 149, loss = 0.03960201\n",
            "Iteration 150, loss = 0.03837968\n",
            "Iteration 151, loss = 0.03718756\n",
            "Iteration 152, loss = 0.03600374\n",
            "Iteration 153, loss = 0.03486670\n",
            "Iteration 154, loss = 0.03377271\n",
            "Iteration 155, loss = 0.03268592\n",
            "Iteration 156, loss = 0.03164360\n",
            "Iteration 157, loss = 0.03061860\n",
            "Iteration 158, loss = 0.02965271\n",
            "Iteration 159, loss = 0.02870259\n",
            "Iteration 160, loss = 0.02776962\n",
            "Iteration 161, loss = 0.02682359\n",
            "Iteration 162, loss = 0.02575344\n",
            "Iteration 163, loss = 0.02484133\n",
            "Iteration 164, loss = 0.02402461\n",
            "Iteration 165, loss = 0.02322012\n",
            "Iteration 166, loss = 0.02247176\n",
            "Iteration 167, loss = 0.02170205\n",
            "Iteration 168, loss = 0.02098954\n",
            "Iteration 169, loss = 0.02027632\n",
            "Iteration 170, loss = 0.01961871\n",
            "Iteration 171, loss = 0.01897560\n",
            "Iteration 172, loss = 0.01836159\n",
            "Iteration 173, loss = 0.01776496\n",
            "Iteration 174, loss = 0.01718606\n",
            "Iteration 175, loss = 0.01664021\n",
            "Iteration 176, loss = 0.01610439\n",
            "Iteration 177, loss = 0.01560400\n",
            "Iteration 178, loss = 0.01511227\n",
            "Iteration 179, loss = 0.01465200\n",
            "Iteration 180, loss = 0.01419693\n",
            "Iteration 181, loss = 0.01376895\n",
            "Iteration 182, loss = 0.01334826\n",
            "Iteration 183, loss = 0.01295341\n",
            "Iteration 184, loss = 0.01256821\n",
            "Iteration 185, loss = 0.01220596\n",
            "Iteration 186, loss = 0.01185582\n",
            "Iteration 187, loss = 0.01152260\n",
            "Iteration 188, loss = 0.01119677\n",
            "Iteration 189, loss = 0.01088603\n",
            "Iteration 190, loss = 0.01058780\n",
            "Iteration 191, loss = 0.01030270\n",
            "Iteration 192, loss = 0.01002892\n",
            "Iteration 193, loss = 0.00976603\n",
            "Iteration 194, loss = 0.00951341\n",
            "Iteration 195, loss = 0.00926931\n",
            "Iteration 196, loss = 0.00903566\n",
            "Iteration 197, loss = 0.00880984\n",
            "Iteration 198, loss = 0.00859397\n",
            "Iteration 199, loss = 0.00838511\n",
            "Iteration 200, loss = 0.00818554\n",
            "Iteration 201, loss = 0.00799273\n",
            "Iteration 202, loss = 0.00780694\n",
            "Iteration 203, loss = 0.00762836\n",
            "Iteration 204, loss = 0.00745591\n",
            "Iteration 205, loss = 0.00729018\n",
            "Iteration 206, loss = 0.00713001\n",
            "Iteration 207, loss = 0.00697630\n",
            "Iteration 208, loss = 0.00682725\n",
            "Iteration 209, loss = 0.00668345\n",
            "Iteration 210, loss = 0.00654488\n",
            "Iteration 211, loss = 0.00641099\n",
            "Iteration 212, loss = 0.00628180\n",
            "Iteration 213, loss = 0.00615663\n",
            "Iteration 214, loss = 0.00603591\n",
            "Iteration 215, loss = 0.00591912\n",
            "Iteration 216, loss = 0.00580612\n",
            "Iteration 217, loss = 0.00569700\n",
            "Iteration 218, loss = 0.00559115\n",
            "Iteration 219, loss = 0.00548890\n",
            "Iteration 220, loss = 0.00538976\n",
            "Iteration 221, loss = 0.00529366\n",
            "Iteration 222, loss = 0.00520097\n",
            "Iteration 223, loss = 0.00511151\n",
            "Iteration 224, loss = 0.00502446\n",
            "Iteration 225, loss = 0.00493986\n",
            "Iteration 226, loss = 0.00485811\n",
            "Iteration 227, loss = 0.00477871\n",
            "Iteration 228, loss = 0.00470154\n",
            "Iteration 229, loss = 0.00462667\n",
            "Iteration 230, loss = 0.00455379\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Distribu»õie predic»õii: (array([0, 1]), array([2, 6]))\n",
            "\n",
            "Clasificare pe setul de test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.25      0.33         4\n",
            "           1       0.50      0.75      0.60         4\n",
            "\n",
            "    accuracy                           0.50         8\n",
            "   macro avg       0.50      0.50      0.47         8\n",
            "weighted avg       0.50      0.50      0.47         8\n",
            "\n",
            "Acurate»õe: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "class SimpleANN:\n",
        "    def __init__(self, input_size, hidden_size=64, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, 1) * 0.01\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = sigmoid(self.Z1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = y.shape[0]\n",
        "        # Reshape y ca sa fie dimensiuni corecte pt extragere\n",
        "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
        "        dW2 = self.A1.T @ dZ2 / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * sigmoid_deriv(self.Z1)\n",
        "        dW1 = X.T @ dZ1 / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Actualizare\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for i in range(epochs):\n",
        "            y_hat = self.forward(X)\n",
        "            self.backward(X, y)\n",
        "            if i % 10 == 0:\n",
        "                loss = -np.mean(y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8))\n",
        "                print(f\"Epoch {i} - Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "h3KmCiXRa1GJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_dataset(\"images/normal\", \"images/sepia\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SimpleANN(input_size=X.shape[1], hidden_size=64, learning_rate=0.1)\n",
        "model.train(X_train, y_train, epochs=100)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciDoSuCnasuS",
        "outputId": "faadd593-ace4-447c-bd88-bdb85f4e7941"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6929\n",
            "Epoch 10 - Loss: 0.6913\n",
            "Epoch 20 - Loss: 0.6915\n",
            "Epoch 30 - Loss: 0.6920\n",
            "Epoch 40 - Loss: 0.6930\n",
            "Epoch 50 - Loss: 0.6947\n",
            "Epoch 60 - Loss: 0.6981\n",
            "Epoch 70 - Loss: 0.7052\n",
            "Epoch 80 - Loss: 0.7195\n",
            "Epoch 90 - Loss: 0.7448\n",
            "Test accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# functii de activar\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_deriv(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "class SimpleANN:\n",
        "    def __init__(self, input_size, hidden_size=64, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, 1) * 0.01\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = relu(self.Z1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = sigmoid(self.Z2)  # strat ie»ôire cu sigmoid\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = y.shape[0]\n",
        "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
        "        dW2 = self.A1.T @ dZ2 / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * relu_deriv(self.Z1)\n",
        "        dW1 = X.T @ dZ1 / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # actualizare\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for i in range(epochs):\n",
        "            y_hat = self.forward(X)\n",
        "            self.backward(X, y)\n",
        "            if i % 10 == 0:\n",
        "                loss = -np.mean(y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8))\n",
        "                print(f\"Epoch {i} - Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "0KhgQyAjJfBW"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_dataset(\"images/normal\", \"images/sepia\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SimpleANN(input_size=X.shape[1], hidden_size=64, learning_rate=0.1)\n",
        "model.train(X_train, y_train, epochs=100)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSOakvrxJ5Ak",
        "outputId": "ac445e0a-4c8c-4e22-bfd0-1351e1d5876b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6944\n",
            "Epoch 10 - Loss: 0.6951\n",
            "Epoch 20 - Loss: 0.7022\n",
            "Epoch 30 - Loss: 0.7012\n",
            "Epoch 40 - Loss: 0.6938\n",
            "Epoch 50 - Loss: 0.6937\n",
            "Epoch 60 - Loss: 0.7296\n",
            "Epoch 70 - Loss: 0.6998\n",
            "Epoch 80 - Loss: 0.6964\n",
            "Epoch 90 - Loss: 0.6959\n",
            "Test accuracy: 0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(X_train, y_train, epochs=500)\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0TZj0PCbHjx",
        "outputId": "9974f257-fc8a-459e-e631-2d83db4648fe"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.7645\n",
            "Epoch 10 - Loss: 0.7104\n",
            "Epoch 20 - Loss: 0.7101\n",
            "Epoch 30 - Loss: 0.7119\n",
            "Epoch 40 - Loss: 0.7178\n",
            "Epoch 50 - Loss: 0.7218\n",
            "Epoch 60 - Loss: 0.7176\n",
            "Epoch 70 - Loss: 0.7192\n",
            "Epoch 80 - Loss: 0.7277\n",
            "Epoch 90 - Loss: 0.7278\n",
            "Epoch 100 - Loss: 0.7270\n",
            "Epoch 110 - Loss: 0.7333\n",
            "Epoch 120 - Loss: 0.7315\n",
            "Epoch 130 - Loss: 0.7275\n",
            "Epoch 140 - Loss: 0.7339\n",
            "Epoch 150 - Loss: 0.7387\n",
            "Epoch 160 - Loss: 0.7347\n",
            "Epoch 170 - Loss: 0.7394\n",
            "Epoch 180 - Loss: 0.7335\n",
            "Epoch 190 - Loss: 0.7386\n",
            "Epoch 200 - Loss: 0.7426\n",
            "Epoch 210 - Loss: 0.7351\n",
            "Epoch 220 - Loss: 0.7400\n",
            "Epoch 230 - Loss: 0.7438\n",
            "Epoch 240 - Loss: 0.7470\n",
            "Epoch 250 - Loss: 0.7386\n",
            "Epoch 260 - Loss: 0.7428\n",
            "Epoch 270 - Loss: 0.7462\n",
            "Epoch 280 - Loss: 0.7491\n",
            "Epoch 290 - Loss: 0.7516\n",
            "Epoch 300 - Loss: 0.7427\n",
            "Epoch 310 - Loss: 0.7462\n",
            "Epoch 320 - Loss: 0.7492\n",
            "Epoch 330 - Loss: 0.7517\n",
            "Epoch 340 - Loss: 0.7539\n",
            "Epoch 350 - Loss: 0.7559\n",
            "Epoch 360 - Loss: 0.7461\n",
            "Epoch 370 - Loss: 0.7491\n",
            "Epoch 380 - Loss: 0.7517\n",
            "Epoch 390 - Loss: 0.7540\n",
            "Epoch 400 - Loss: 0.7560\n",
            "Epoch 410 - Loss: 0.7578\n",
            "Epoch 420 - Loss: 0.7595\n",
            "Epoch 430 - Loss: 0.7483\n",
            "Epoch 440 - Loss: 0.7511\n",
            "Epoch 450 - Loss: 0.7535\n",
            "Epoch 460 - Loss: 0.7556\n",
            "Epoch 470 - Loss: 0.7575\n",
            "Epoch 480 - Loss: 0.7593\n",
            "Epoch 490 - Loss: 0.7608\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cod propriu ANN"
      ],
      "metadata": {
        "id": "goYQYcegdkAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_images_from_folder(folder, label, size=(32, 32)):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder):\n",
        "        path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB').resize(size)\n",
        "            data.append((np.array(img).flatten() / 255.0, label))\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "def load_dataset():\n",
        "    normal = load_images_from_folder('images/normal', 0)\n",
        "    sepia = load_images_from_folder('images/sepia', 1)\n",
        "    dataset = normal + sepia\n",
        "    np.random.shuffle(dataset)\n",
        "    X, y = zip(*dataset)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "class ANN:\n",
        "    def __init__(self, input_size, hidden_size=64, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, 1) * 0.01\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_deriv(self, z):\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = self.sigmoid(self.Z1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = self.sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_loss(self, y_hat, y):\n",
        "        y_hat = np.clip(y_hat, 1e-8, 1 - 1e-8)\n",
        "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
        "        dW2 = self.A1.T @ dZ2 / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * self.sigmoid_deriv(self.Z1)\n",
        "        dW1 = X.T @ dZ1 / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        #actualizare param\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            y_hat = self.forward(X)\n",
        "            loss = self.compute_loss(y_hat, y)\n",
        "            self.backward(X, y)\n",
        "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_hat = self.forward(X)\n",
        "        return (y_hat > 0.5).astype(int).flatten()\n",
        "\n",
        "\n",
        "\n",
        "X, y = load_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = ANN(input_size=X.shape[1], hidden_size=64, learning_rate=0.01)\n",
        "model.train(X_train, y_train, epochs=200)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQn0RqmWdmh5",
        "outputId": "96cde5c9-f42d-4500-ff9b-db6ad3e92837"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6932\n",
            "Epoch 10 - Loss: 0.6932\n",
            "Epoch 20 - Loss: 0.6932\n",
            "Epoch 30 - Loss: 0.6932\n",
            "Epoch 40 - Loss: 0.6932\n",
            "Epoch 50 - Loss: 0.6932\n",
            "Epoch 60 - Loss: 0.6932\n",
            "Epoch 70 - Loss: 0.6932\n",
            "Epoch 80 - Loss: 0.6932\n",
            "Epoch 90 - Loss: 0.6932\n",
            "Epoch 100 - Loss: 0.6932\n",
            "Epoch 110 - Loss: 0.6932\n",
            "Epoch 120 - Loss: 0.6932\n",
            "Epoch 130 - Loss: 0.6932\n",
            "Epoch 140 - Loss: 0.6932\n",
            "Epoch 150 - Loss: 0.6932\n",
            "Epoch 160 - Loss: 0.6932\n",
            "Epoch 170 - Loss: 0.6932\n",
            "Epoch 180 - Loss: 0.6932\n",
            "Epoch 190 - Loss: 0.6932\n",
            "Epoch 199 - Loss: 0.6932\n",
            "Test accuracy: 0.5833333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "influenta (hyper)parametrilor"
      ],
      "metadata": {
        "id": "c6VdShgygTe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(hidden_size=64, learning_rate=0.01, epochs=200):\n",
        "    X, y = load_dataset()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "    model = ANN(input_size=X.shape[1], hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    model.train(X_train, y_train, epochs=epochs)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = np.mean(y_pred == y_test)\n",
        "    print(f\"Hidden={hidden_size}, LR={learning_rate}, Epochs={epochs} ‚Üí Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "qOt_F5-we-Yr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for h in [4, 16, 32, 64, 128]:\n",
        "    run_experiment(hidden_size=h)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg8w2dpGfBur",
        "outputId": "6f34af35-e70c-4e3a-ad00-c82b87a17eaa"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6938\n",
            "Epoch 10 - Loss: 0.6915\n",
            "Epoch 20 - Loss: 0.6895\n",
            "Epoch 30 - Loss: 0.6877\n",
            "Epoch 40 - Loss: 0.6860\n",
            "Epoch 50 - Loss: 0.6844\n",
            "Epoch 60 - Loss: 0.6830\n",
            "Epoch 70 - Loss: 0.6817\n",
            "Epoch 80 - Loss: 0.6804\n",
            "Epoch 90 - Loss: 0.6793\n",
            "Epoch 100 - Loss: 0.6783\n",
            "Epoch 110 - Loss: 0.6774\n",
            "Epoch 120 - Loss: 0.6766\n",
            "Epoch 130 - Loss: 0.6759\n",
            "Epoch 140 - Loss: 0.6752\n",
            "Epoch 150 - Loss: 0.6746\n",
            "Epoch 160 - Loss: 0.6741\n",
            "Epoch 170 - Loss: 0.6736\n",
            "Epoch 180 - Loss: 0.6732\n",
            "Epoch 190 - Loss: 0.6728\n",
            "Epoch 199 - Loss: 0.6725\n",
            "Hidden=4, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.2500\n",
            "Epoch 0 - Loss: 0.6946\n",
            "Epoch 10 - Loss: 0.6919\n",
            "Epoch 20 - Loss: 0.6899\n",
            "Epoch 30 - Loss: 0.6883\n",
            "Epoch 40 - Loss: 0.6870\n",
            "Epoch 50 - Loss: 0.6860\n",
            "Epoch 60 - Loss: 0.6852\n",
            "Epoch 70 - Loss: 0.6847\n",
            "Epoch 80 - Loss: 0.6842\n",
            "Epoch 90 - Loss: 0.6838\n",
            "Epoch 100 - Loss: 0.6836\n",
            "Epoch 110 - Loss: 0.6834\n",
            "Epoch 120 - Loss: 0.6832\n",
            "Epoch 130 - Loss: 0.6831\n",
            "Epoch 140 - Loss: 0.6831\n",
            "Epoch 150 - Loss: 0.6830\n",
            "Epoch 160 - Loss: 0.6830\n",
            "Epoch 170 - Loss: 0.6829\n",
            "Epoch 180 - Loss: 0.6829\n",
            "Epoch 190 - Loss: 0.6829\n",
            "Epoch 199 - Loss: 0.6829\n",
            "Hidden=16, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.3333\n",
            "Epoch 0 - Loss: 0.6919\n",
            "Epoch 10 - Loss: 0.6886\n",
            "Epoch 20 - Loss: 0.6865\n",
            "Epoch 30 - Loss: 0.6852\n",
            "Epoch 40 - Loss: 0.6844\n",
            "Epoch 50 - Loss: 0.6839\n",
            "Epoch 60 - Loss: 0.6836\n",
            "Epoch 70 - Loss: 0.6833\n",
            "Epoch 80 - Loss: 0.6832\n",
            "Epoch 90 - Loss: 0.6831\n",
            "Epoch 100 - Loss: 0.6831\n",
            "Epoch 110 - Loss: 0.6830\n",
            "Epoch 120 - Loss: 0.6830\n",
            "Epoch 130 - Loss: 0.6830\n",
            "Epoch 140 - Loss: 0.6830\n",
            "Epoch 150 - Loss: 0.6830\n",
            "Epoch 160 - Loss: 0.6830\n",
            "Epoch 170 - Loss: 0.6830\n",
            "Epoch 180 - Loss: 0.6830\n",
            "Epoch 190 - Loss: 0.6830\n",
            "Epoch 199 - Loss: 0.6830\n",
            "Hidden=32, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.3333\n",
            "Epoch 0 - Loss: 0.6936\n",
            "Epoch 10 - Loss: 0.6934\n",
            "Epoch 20 - Loss: 0.6933\n",
            "Epoch 30 - Loss: 0.6932\n",
            "Epoch 40 - Loss: 0.6932\n",
            "Epoch 50 - Loss: 0.6932\n",
            "Epoch 60 - Loss: 0.6932\n",
            "Epoch 70 - Loss: 0.6932\n",
            "Epoch 80 - Loss: 0.6932\n",
            "Epoch 90 - Loss: 0.6932\n",
            "Epoch 100 - Loss: 0.6932\n",
            "Epoch 110 - Loss: 0.6932\n",
            "Epoch 120 - Loss: 0.6932\n",
            "Epoch 130 - Loss: 0.6932\n",
            "Epoch 140 - Loss: 0.6932\n",
            "Epoch 150 - Loss: 0.6932\n",
            "Epoch 160 - Loss: 0.6932\n",
            "Epoch 170 - Loss: 0.6932\n",
            "Epoch 180 - Loss: 0.6932\n",
            "Epoch 190 - Loss: 0.6932\n",
            "Epoch 199 - Loss: 0.6932\n",
            "Hidden=64, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.5833\n",
            "Epoch 0 - Loss: 0.6861\n",
            "Epoch 10 - Loss: 0.6725\n",
            "Epoch 20 - Loss: 0.6704\n",
            "Epoch 30 - Loss: 0.6701\n",
            "Epoch 40 - Loss: 0.6701\n",
            "Epoch 50 - Loss: 0.6701\n",
            "Epoch 60 - Loss: 0.6701\n",
            "Epoch 70 - Loss: 0.6702\n",
            "Epoch 80 - Loss: 0.6702\n",
            "Epoch 90 - Loss: 0.6702\n",
            "Epoch 100 - Loss: 0.6703\n",
            "Epoch 110 - Loss: 0.6703\n",
            "Epoch 120 - Loss: 0.6703\n",
            "Epoch 130 - Loss: 0.6704\n",
            "Epoch 140 - Loss: 0.6704\n",
            "Epoch 150 - Loss: 0.6705\n",
            "Epoch 160 - Loss: 0.6706\n",
            "Epoch 170 - Loss: 0.6706\n",
            "Epoch 180 - Loss: 0.6707\n",
            "Epoch 190 - Loss: 0.6708\n",
            "Epoch 199 - Loss: 0.6709\n",
            "Hidden=128, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
        "    run_experiment(learning_rate=lr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11zLmzGzfdyC",
        "outputId": "3cdd7b10-84b5-4767-addd-64eeee79be66"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6905\n",
            "Epoch 10 - Loss: 0.6904\n",
            "Epoch 20 - Loss: 0.6903\n",
            "Epoch 30 - Loss: 0.6903\n",
            "Epoch 40 - Loss: 0.6902\n",
            "Epoch 50 - Loss: 0.6901\n",
            "Epoch 60 - Loss: 0.6901\n",
            "Epoch 70 - Loss: 0.6900\n",
            "Epoch 80 - Loss: 0.6899\n",
            "Epoch 90 - Loss: 0.6899\n",
            "Epoch 100 - Loss: 0.6898\n",
            "Epoch 110 - Loss: 0.6897\n",
            "Epoch 120 - Loss: 0.6897\n",
            "Epoch 130 - Loss: 0.6896\n",
            "Epoch 140 - Loss: 0.6896\n",
            "Epoch 150 - Loss: 0.6895\n",
            "Epoch 160 - Loss: 0.6894\n",
            "Epoch 170 - Loss: 0.6894\n",
            "Epoch 180 - Loss: 0.6893\n",
            "Epoch 190 - Loss: 0.6893\n",
            "Epoch 199 - Loss: 0.6892\n",
            "Hidden=64, LR=0.0001, Epochs=200 ‚Üí Accuracy: 0.3333\n",
            "Epoch 0 - Loss: 0.6931\n",
            "Epoch 10 - Loss: 0.6929\n",
            "Epoch 20 - Loss: 0.6927\n",
            "Epoch 30 - Loss: 0.6925\n",
            "Epoch 40 - Loss: 0.6923\n",
            "Epoch 50 - Loss: 0.6922\n",
            "Epoch 60 - Loss: 0.6920\n",
            "Epoch 70 - Loss: 0.6919\n",
            "Epoch 80 - Loss: 0.6918\n",
            "Epoch 90 - Loss: 0.6917\n",
            "Epoch 100 - Loss: 0.6916\n",
            "Epoch 110 - Loss: 0.6915\n",
            "Epoch 120 - Loss: 0.6914\n",
            "Epoch 130 - Loss: 0.6913\n",
            "Epoch 140 - Loss: 0.6913\n",
            "Epoch 150 - Loss: 0.6912\n",
            "Epoch 160 - Loss: 0.6912\n",
            "Epoch 170 - Loss: 0.6911\n",
            "Epoch 180 - Loss: 0.6911\n",
            "Epoch 190 - Loss: 0.6910\n",
            "Epoch 199 - Loss: 0.6910\n",
            "Hidden=64, LR=0.001, Epochs=200 ‚Üí Accuracy: 0.4167\n",
            "Epoch 0 - Loss: 0.6941\n",
            "Epoch 10 - Loss: 0.6918\n",
            "Epoch 20 - Loss: 0.6910\n",
            "Epoch 30 - Loss: 0.6907\n",
            "Epoch 40 - Loss: 0.6906\n",
            "Epoch 50 - Loss: 0.6906\n",
            "Epoch 60 - Loss: 0.6906\n",
            "Epoch 70 - Loss: 0.6906\n",
            "Epoch 80 - Loss: 0.6906\n",
            "Epoch 90 - Loss: 0.6906\n",
            "Epoch 100 - Loss: 0.6907\n",
            "Epoch 110 - Loss: 0.6907\n",
            "Epoch 120 - Loss: 0.6907\n",
            "Epoch 130 - Loss: 0.6907\n",
            "Epoch 140 - Loss: 0.6907\n",
            "Epoch 150 - Loss: 0.6907\n",
            "Epoch 160 - Loss: 0.6907\n",
            "Epoch 170 - Loss: 0.6907\n",
            "Epoch 180 - Loss: 0.6907\n",
            "Epoch 190 - Loss: 0.6908\n",
            "Epoch 199 - Loss: 0.6908\n",
            "Hidden=64, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.4167\n",
            "Epoch 0 - Loss: 0.6952\n",
            "Epoch 10 - Loss: 0.6906\n",
            "Epoch 20 - Loss: 0.6906\n",
            "Epoch 30 - Loss: 0.6907\n",
            "Epoch 40 - Loss: 0.6909\n",
            "Epoch 50 - Loss: 0.6912\n",
            "Epoch 60 - Loss: 0.6916\n",
            "Epoch 70 - Loss: 0.6923\n",
            "Epoch 80 - Loss: 0.6932\n",
            "Epoch 90 - Loss: 0.6945\n",
            "Epoch 100 - Loss: 0.6960\n",
            "Epoch 110 - Loss: 0.6980\n",
            "Epoch 120 - Loss: 0.7005\n",
            "Epoch 130 - Loss: 0.7036\n",
            "Epoch 140 - Loss: 0.7075\n",
            "Epoch 150 - Loss: 0.7125\n",
            "Epoch 160 - Loss: 0.7187\n",
            "Epoch 170 - Loss: 0.7267\n",
            "Epoch 180 - Loss: 0.7366\n",
            "Epoch 190 - Loss: 0.7485\n",
            "Epoch 199 - Loss: 0.7610\n",
            "Hidden=64, LR=0.1, Epochs=200 ‚Üí Accuracy: 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for e in [50, 100, 200, 500]:\n",
        "    run_experiment(epochs=e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y2YuWXQffCS",
        "outputId": "a1556a27-a252-4187-9abe-98636ab6f94e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6922\n",
            "Epoch 10 - Loss: 0.6869\n",
            "Epoch 20 - Loss: 0.6846\n",
            "Epoch 30 - Loss: 0.6837\n",
            "Epoch 40 - Loss: 0.6833\n",
            "Epoch 49 - Loss: 0.6831\n",
            "Hidden=64, LR=0.01, Epochs=50 ‚Üí Accuracy: 0.3333\n",
            "Epoch 0 - Loss: 0.6937\n",
            "Epoch 10 - Loss: 0.6919\n",
            "Epoch 20 - Loss: 0.6912\n",
            "Epoch 30 - Loss: 0.6909\n",
            "Epoch 40 - Loss: 0.6907\n",
            "Epoch 50 - Loss: 0.6906\n",
            "Epoch 60 - Loss: 0.6906\n",
            "Epoch 70 - Loss: 0.6906\n",
            "Epoch 80 - Loss: 0.6906\n",
            "Epoch 90 - Loss: 0.6906\n",
            "Epoch 99 - Loss: 0.6906\n",
            "Hidden=64, LR=0.01, Epochs=100 ‚Üí Accuracy: 0.4167\n",
            "Epoch 0 - Loss: 0.6940\n",
            "Epoch 10 - Loss: 0.6920\n",
            "Epoch 20 - Loss: 0.6912\n",
            "Epoch 30 - Loss: 0.6908\n",
            "Epoch 40 - Loss: 0.6907\n",
            "Epoch 50 - Loss: 0.6906\n",
            "Epoch 60 - Loss: 0.6906\n",
            "Epoch 70 - Loss: 0.6906\n",
            "Epoch 80 - Loss: 0.6906\n",
            "Epoch 90 - Loss: 0.6906\n",
            "Epoch 100 - Loss: 0.6906\n",
            "Epoch 110 - Loss: 0.6906\n",
            "Epoch 120 - Loss: 0.6906\n",
            "Epoch 130 - Loss: 0.6906\n",
            "Epoch 140 - Loss: 0.6906\n",
            "Epoch 150 - Loss: 0.6906\n",
            "Epoch 160 - Loss: 0.6906\n",
            "Epoch 170 - Loss: 0.6906\n",
            "Epoch 180 - Loss: 0.6906\n",
            "Epoch 190 - Loss: 0.6906\n",
            "Epoch 199 - Loss: 0.6906\n",
            "Hidden=64, LR=0.01, Epochs=200 ‚Üí Accuracy: 0.4167\n",
            "Epoch 0 - Loss: 0.6911\n",
            "Epoch 10 - Loss: 0.6790\n",
            "Epoch 20 - Loss: 0.6739\n",
            "Epoch 30 - Loss: 0.6717\n",
            "Epoch 40 - Loss: 0.6708\n",
            "Epoch 50 - Loss: 0.6704\n",
            "Epoch 60 - Loss: 0.6702\n",
            "Epoch 70 - Loss: 0.6701\n",
            "Epoch 80 - Loss: 0.6701\n",
            "Epoch 90 - Loss: 0.6700\n",
            "Epoch 100 - Loss: 0.6700\n",
            "Epoch 110 - Loss: 0.6701\n",
            "Epoch 120 - Loss: 0.6701\n",
            "Epoch 130 - Loss: 0.6701\n",
            "Epoch 140 - Loss: 0.6701\n",
            "Epoch 150 - Loss: 0.6701\n",
            "Epoch 160 - Loss: 0.6701\n",
            "Epoch 170 - Loss: 0.6701\n",
            "Epoch 180 - Loss: 0.6702\n",
            "Epoch 190 - Loss: 0.6702\n",
            "Epoch 200 - Loss: 0.6702\n",
            "Epoch 210 - Loss: 0.6703\n",
            "Epoch 220 - Loss: 0.6703\n",
            "Epoch 230 - Loss: 0.6703\n",
            "Epoch 240 - Loss: 0.6704\n",
            "Epoch 250 - Loss: 0.6704\n",
            "Epoch 260 - Loss: 0.6705\n",
            "Epoch 270 - Loss: 0.6705\n",
            "Epoch 280 - Loss: 0.6706\n",
            "Epoch 290 - Loss: 0.6707\n",
            "Epoch 300 - Loss: 0.6707\n",
            "Epoch 310 - Loss: 0.6708\n",
            "Epoch 320 - Loss: 0.6709\n",
            "Epoch 330 - Loss: 0.6710\n",
            "Epoch 340 - Loss: 0.6711\n",
            "Epoch 350 - Loss: 0.6711\n",
            "Epoch 360 - Loss: 0.6712\n",
            "Epoch 370 - Loss: 0.6713\n",
            "Epoch 380 - Loss: 0.6715\n",
            "Epoch 390 - Loss: 0.6716\n",
            "Epoch 400 - Loss: 0.6717\n",
            "Epoch 410 - Loss: 0.6718\n",
            "Epoch 420 - Loss: 0.6719\n",
            "Epoch 430 - Loss: 0.6721\n",
            "Epoch 440 - Loss: 0.6722\n",
            "Epoch 450 - Loss: 0.6724\n",
            "Epoch 460 - Loss: 0.6725\n",
            "Epoch 470 - Loss: 0.6727\n",
            "Epoch 480 - Loss: 0.6729\n",
            "Epoch 490 - Loss: 0.6730\n",
            "Epoch 499 - Loss: 0.6732\n",
            "Hidden=64, LR=0.01, Epochs=500 ‚Üí Accuracy: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN cod propriu"
      ],
      "metadata": {
        "id": "_Yf-svqZgVlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_images_from_folder(folder, label, size=(32, 32)):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder):\n",
        "        path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB').resize(size)\n",
        "            data.append((np.array(img) / 255.0, label))\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "def load_dataset():\n",
        "    data = load_images_from_folder(\"images/normal\", 0) + load_images_from_folder(\"images/sepia\", 1)\n",
        "    np.random.shuffle(data)\n",
        "    X, y = zip(*data)\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "id": "5AsYuXQygYR-"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve2d(img, kernel):\n",
        "    h, w = img.shape\n",
        "    kh, kw = kernel.shape\n",
        "    out = np.zeros((h - kh + 1, w - kw + 1))\n",
        "    for i in range(out.shape[0]):\n",
        "        for j in range(out.shape[1]):\n",
        "            out[i, j] = np.sum(img[i:i+kh, j:j+kw] * kernel)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "NeTmBrTcgemG"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN:\n",
        "    def __init__(self):\n",
        "        # Kerneluri random (3x3) pentru fiecare canal\n",
        "        self.kernel_r = np.random.randn(3, 3)\n",
        "        self.kernel_g = np.random.randn(3, 3)\n",
        "        self.kernel_b = np.random.randn(3, 3)\n",
        "        self.W = np.random.randn(30*30, 1) * 0.01  # After conv and flatten, adjusted size to 30x30\n",
        "        self.b = np.zeros((1,))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        conv_r = np.array([convolve2d(img[:, :, 0], self.kernel_r) for img in X])\n",
        "        conv_g = np.array([convolve2d(img[:, :, 1], self.kernel_g) for img in X])\n",
        "        conv_b = np.array([convolve2d(img[:, :, 2], self.kernel_b) for img in X])\n",
        "        conv_total = self.relu(conv_r + conv_g + conv_b)  # Shape: (n_samples, 30, 30)\n",
        "\n",
        "        flat = conv_total.reshape(len(X), -1)  # Flatten (n_samples, 900) # Adjusted to (n_samples, 30x30=900)\n",
        "        z = flat @ self.W + self.b\n",
        "        return 1 / (1 + np.exp(-z))  # sigmoid\n",
        "\n",
        "    def compute_loss(self, y_hat, y):\n",
        "        y_hat = np.clip(y_hat, 1e-8, 1 - 1e-8)\n",
        "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "    def backward(self, X, y, y_hat, learning_rate=0.01):\n",
        "        m = y.shape[0]\n",
        "        dz = y_hat - y.reshape(-1, 1)\n",
        "        conv_r = np.array([convolve2d(img[:, :, 0], self.kernel_r) for img in X])\n",
        "        conv_g = np.array([convolve2d(img[:, :, 1], self.kernel_g) for img in X])\n",
        "        conv_b = np.array([convolve2d(img[:, :, 2], self.kernel_b) for img in X])\n",
        "        conv_total = self.relu(conv_r + conv_g + conv_b)\n",
        "        flat = conv_total.reshape(m, -1)\n",
        "\n",
        "        dW = flat.T @ dz / m\n",
        "        db = np.sum(dz) / m\n",
        "\n",
        "        self.W -= learning_rate * dW\n",
        "        self.b -= learning_rate * db\n",
        "\n",
        "    def train(self, X, y, epochs=50, learning_rate=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            y_hat = self.forward(X)\n",
        "            loss = self.compute_loss(y_hat, y)\n",
        "            self.backward(X, y, y_hat, learning_rate)\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_hat = self.forward(X)\n",
        "        return (y_hat > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "t_8cnheaghmb"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "cnn = SimpleCNN()\n",
        "cnn.train(X_train, y_train, epochs=100, learning_rate=0.01)\n",
        "\n",
        "y_pred = cnn.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyVyJg9tgk-I",
        "outputId": "6d06629f-9e39-41d1-d3f8-0d18e78db87a"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7744\n",
            "Epoch 10, Loss: 3.0577\n",
            "Epoch 20, Loss: 7.4834\n",
            "Epoch 30, Loss: 2.6251\n",
            "Epoch 40, Loss: 2.3224\n",
            "Epoch 50, Loss: 8.7485\n",
            "Epoch 60, Loss: 3.9691\n",
            "Epoch 70, Loss: 4.7558\n",
            "Epoch 80, Loss: 7.7348\n",
            "Epoch 90, Loss: 3.7317\n",
            "Test Accuracy: 0.8750\n"
          ]
        }
      ]
    }
  ]
}